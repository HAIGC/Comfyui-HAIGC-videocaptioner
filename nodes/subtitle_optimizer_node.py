"""
å­—å¹•ä¼˜åŒ–èŠ‚ç‚¹ - æä¾›å¼ºå¤§çš„å­—å¹•åˆ†æ®µå’Œä¼˜åŒ–åŠŸèƒ½
æ”¯æŒå¤šç§ä¼˜åŒ–æ¨¡å¼å’Œè‡ªå®šä¹‰å‚æ•°
"""

import sys
from pathlib import Path
from typing import Tuple, Any

# æ·»åŠ  VideoCaptioner è·¯å¾„
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir / "VideoCaptioner"))

try:
    from app.core.bk_asr.asr_data import ASRData
    from app.core.utils.optimize_subtitles import optimize_subtitles, count_words
except ImportError as e:
    print(f"[SubtitleOptimizer] å¯¼å…¥å¤±è´¥: {e}")
    ASRData = None


class SubtitleOptimizerNode:
    """
    å­—å¹•ä¼˜åŒ–èŠ‚ç‚¹ - æ™ºèƒ½åˆ†æ®µå’Œä¼˜åŒ–
    
    æ”¯æŒå¤šç§ä¼˜åŒ–æ¨¡å¼ï¼š
    - æ™ºèƒ½æ¨¡å¼ï¼šè‡ªåŠ¨ä¼˜åŒ–çŸ­å¥
    - è‡ªå®šä¹‰æ¨¡å¼ï¼šæ‰‹åŠ¨è®¾ç½®å‚æ•°
    - å¹³è¡¡æ¨¡å¼ï¼šåœ¨è´¨é‡å’Œé€Ÿåº¦é—´å¹³è¡¡
    - æ¿€è¿›æ¨¡å¼ï¼šæœ€å¤§åŒ–åˆå¹¶
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å­—å¹•æ•°æ®": ("SUBTITLE_DATA",),  # è¾“å…¥å­—å¹•æ•°æ®
                "ä¼˜åŒ–æ¨¡å¼": ([
                    "æ™ºèƒ½æ¨¡å¼ âœ¨",
                    "æŒ‰å­—æ•°é™åˆ¶ ğŸ“",
                    "è‡ªå®šä¹‰æ¨¡å¼ ğŸ›ï¸", 
                    "å¹³è¡¡æ¨¡å¼ âš–ï¸",
                    "æ¿€è¿›æ¨¡å¼ ğŸš€",
                    "å…³é—­ä¼˜åŒ– âŒ"
                ], {
                    "default": "æ™ºèƒ½æ¨¡å¼ âœ¨"
                }),
                # è‡ªå®šä¹‰å‚æ•°ï¼ˆä»…åœ¨è‡ªå®šä¹‰æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰
                "æœ€å°è¯æ•°é˜ˆå€¼": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 20,
                    "step": 1,
                    "display": "number",
                    "tooltip": "çŸ­å¥é˜ˆå€¼ï¼šè¯æ•°â‰¤æ­¤å€¼ä¼šå°è¯•åˆå¹¶"
                }),
                "æœ€å¤§è¯æ•°é™åˆ¶": ("INT", {
                    "default": 10,
                    "min": 5,
                    "max": 50,
                    "step": 1,
                    "display": "number",
                    "tooltip": "é•¿å¥é™åˆ¶ï¼šåˆå¹¶åä¸è¶…è¿‡æ­¤è¯æ•°"
                }),
                "æ—¶é—´é—´éš”": ("INT", {
                    "default": 300,
                    "min": 0,
                    "max": 2000,
                    "step": 10,
                    "display": "number",
                    "tooltip": "æ—¶é—´é—´éš”é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰ï¼šæ™ºèƒ½æ¨¡å¼=åˆå¹¶é˜ˆå€¼ï¼ŒæŒ‰åœé¡¿åˆ†æ®µ=åœé¡¿é˜ˆå€¼"
                }),
                "åˆå¹¶ç­–ç•¥": ([
                    "ä¿å®ˆåˆå¹¶",
                    "æ ‡å‡†åˆå¹¶", 
                    "ç§¯æåˆå¹¶"
                ], {
                    "default": "æ ‡å‡†åˆå¹¶"
                }),
                # å­—æ•°é™åˆ¶å‚æ•°ï¼ˆç”¨äº"æŒ‰å­—æ•°é™åˆ¶"æ¨¡å¼ï¼‰
                "æ¯æ®µæœ€å¤§å­—ç¬¦æ•°": ("INT", {
                    "default": 40,
                    "min": 10,
                    "max": 100,
                    "step": 1,
                    "display": "number",
                    "tooltip": "æ¯æ®µæœ€å¤§å­—ç¬¦æ•°ï¼ˆç¡¬é™åˆ¶ï¼‰ï¼Œç¬¦åˆå­—å¹•æ ‡å‡†ï¼šå•è¡Œ15-20å­—ï¼ŒåŒè¡Œ30-40å­—"
                }),
                "åˆ†æ®µé˜ˆå€¼": ("INT", {
                    "default": 35,
                    "min": 10,
                    "max": 90,
                    "step": 1,
                    "display": "number",
                    "tooltip": "è¶…è¿‡æ­¤å­—ç¬¦æ•°æ—¶å¼€å§‹å¯»æ‰¾åˆ†æ®µç‚¹ï¼ˆè½¯é™åˆ¶ï¼‰ï¼Œåº”å°äºæœ€å¤§å­—ç¬¦æ•°"
                }),
            }
        }
    
    RETURN_TYPES = ("SUBTITLE_DATA", "STRING", "STRING", "STRING", "STRING", "STRING", "STRING",)
    RETURN_NAMES = (
        "ä¼˜åŒ–åå­—å¹•",              # ä¼˜åŒ–åçš„å­—å¹•å¯¹è±¡
        "ä¼˜åŒ–æŠ¥å‘Š",             # ä¼˜åŒ–æŠ¥å‘Š
        "å®Œæ•´æ—¶é—´æˆ³æ–‡æœ¬",    # å®Œæ•´æ—¶é—´æˆ³ [HH:MM:SS.mmm]
        "ç®€æ´æ—¶é—´æˆ³æ–‡æœ¬",  # ç®€æ´ç§’æ•° (0.0, 1.5)
        "SRTæ ¼å¼æ–‡æœ¬",        # SRT æ ¼å¼
        "JSONæ ¼å¼æ–‡æœ¬",       # JSON æ ¼å¼
        "CSVæ ¼å¼æ–‡æœ¬",        # CSV æ ¼å¼
    )
    FUNCTION = "optimize"
    CATEGORY = "video/subtitle"
    
    def optimize(
        self,
        å­—å¹•æ•°æ®: Any,
        ä¼˜åŒ–æ¨¡å¼: str,
        æœ€å°è¯æ•°é˜ˆå€¼: int,
        æœ€å¤§è¯æ•°é™åˆ¶: int,
        æ—¶é—´é—´éš”: int,
        åˆå¹¶ç­–ç•¥: str,
        æ¯æ®µæœ€å¤§å­—ç¬¦æ•°: int,
        åˆ†æ®µé˜ˆå€¼: int,
    ) -> Tuple[Any, str, str, str, str, str, str]:
        """
        ä¼˜åŒ–å­—å¹•åˆ†æ®µ
        
        Args:
            subtitle_data: è¾“å…¥çš„å­—å¹•æ•°æ®å¯¹è±¡
            optimize_mode: ä¼˜åŒ–æ¨¡å¼
            min_word_threshold: æœ€å°è¯æ•°é˜ˆå€¼
            max_word_limit: æœ€å¤§è¯æ•°é™åˆ¶
            time_gap_ms: æ—¶é—´é—´éš”é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
            merge_strategy: åˆå¹¶ç­–ç•¥
            
        Returns:
            (ä¼˜åŒ–åçš„å­—å¹•æ•°æ®, ä¼˜åŒ–æŠ¥å‘Š, 5ç§æ ¼å¼åŒ–æ–‡æœ¬)
        """
        
        if not å­—å¹•æ•°æ® or not hasattr(å­—å¹•æ•°æ®, 'segments'):
            empty_str = ""
            return (å­—å¹•æ•°æ®, "é”™è¯¯ï¼šæ— æ•ˆçš„å­—å¹•æ•°æ®", empty_str, empty_str, empty_str, empty_str, empty_str)
        
        # ä¿å­˜åŸå§‹æ®µæ•°
        original_count = len(å­—å¹•æ•°æ®.segments)
        
        # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä¼˜åŒ–
        if "å…³é—­" in ä¼˜åŒ–æ¨¡å¼:
            report = f"âœ‹ ä¼˜åŒ–å·²å…³é—­\nåŸå§‹æ®µæ•°: {original_count}"
            print(f"[SubtitleOptimizer] {report}")
            # ç”Ÿæˆæ ¼å¼åŒ–æ–‡æœ¬
            text_with_timestamp = self._format_text_with_timestamp(å­—å¹•æ•°æ®)
            text_simple_timestamp = self._format_text_with_simple_timestamp(å­—å¹•æ•°æ®)
            text_srt = self._format_text_srt(å­—å¹•æ•°æ®)
            text_json = self._format_text_json(å­—å¹•æ•°æ®)
            text_csv = self._format_text_csv(å­—å¹•æ•°æ®)
            return (å­—å¹•æ•°æ®, report, text_with_timestamp, text_simple_timestamp, text_srt, text_json, text_csv)
        
        # å¤åˆ¶æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        from copy import deepcopy
        optimized_data = deepcopy(å­—å¹•æ•°æ®)
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°
        if "æ™ºèƒ½" in ä¼˜åŒ–æ¨¡å¼:
            # æ™ºèƒ½æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤ä¼˜åŒ–ç®—æ³•
            params = {
                "min_words": 4,
                "max_words": 10,
                "time_gap": 100
            }
            print(f"[SubtitleOptimizer] æ™ºèƒ½æ¨¡å¼: min={params['min_words']}, max={params['max_words']}, gap={params['time_gap']}ms")
            # å…ˆæŒ‰æ ‡ç‚¹åˆ†æ®µï¼Œç¡®ä¿æ¯å¥æˆä¸ºç‹¬ç«‹è¡Œ
            self._split_by_punctuation(optimized_data)
            self._optimize_with_params(optimized_data, params)
            
        elif "å­—æ•°" in ä¼˜åŒ–æ¨¡å¼:
            # æŒ‰å­—æ•°é™åˆ¶åˆ†æ®µæ¨¡å¼ï¼šå…ˆæŒ‰æ ‡ç‚¹åˆ†æ®µï¼Œå†æŒ‰å­—æ•°é™åˆ¶è¿›ä¸€æ­¥åˆ‡åˆ†
            print(f"[SubtitleOptimizer] æŒ‰å­—æ•°é™åˆ¶åˆ†æ®µæ¨¡å¼: max={æ¯æ®µæœ€å¤§å­—ç¬¦æ•°}å­—, threshold={åˆ†æ®µé˜ˆå€¼}å­—")
            self._split_by_char_limit(optimized_data, æ¯æ®µæœ€å¤§å­—ç¬¦æ•°, åˆ†æ®µé˜ˆå€¼)
            
        elif "å¹³è¡¡" in ä¼˜åŒ–æ¨¡å¼:
            # å¹³è¡¡æ¨¡å¼ï¼šé€‚åº¦åˆå¹¶
            params = {
                "min_words": 3,
                "max_words": 12,
                "time_gap": 150
            }
            print(f"[SubtitleOptimizer] å¹³è¡¡æ¨¡å¼: min={params['min_words']}, max={params['max_words']}, gap={params['time_gap']}ms")
            self._optimize_with_params(optimized_data, params)
            
        elif "æ¿€è¿›" in ä¼˜åŒ–æ¨¡å¼:
            # æ¿€è¿›æ¨¡å¼ï¼šæœ€å¤§åŒ–åˆå¹¶
            params = {
                "min_words": 6,
                "max_words": 15,
                "time_gap": 300
            }
            print(f"[SubtitleOptimizer] æ¿€è¿›æ¨¡å¼: min={params['min_words']}, max={params['max_words']}, gap={params['time_gap']}ms")
            self._optimize_with_params(optimized_data, params)
            
        elif "è‡ªå®šä¹‰" in ä¼˜åŒ–æ¨¡å¼:
            # è‡ªå®šä¹‰æ¨¡å¼ï¼šä½¿ç”¨ç”¨æˆ·å‚æ•°
            params = {
                "min_words": æœ€å°è¯æ•°é˜ˆå€¼,
                "max_words": æœ€å¤§è¯æ•°é™åˆ¶,
                "time_gap": æ—¶é—´é—´éš”
            }
            print(f"[SubtitleOptimizer] è‡ªå®šä¹‰æ¨¡å¼: min={params['min_words']}, max={params['max_words']}, gap={params['time_gap']}ms")
            print(f"[SubtitleOptimizer] åˆå¹¶ç­–ç•¥: {åˆå¹¶ç­–ç•¥}")
            
            # æ ¹æ®åˆå¹¶ç­–ç•¥è°ƒæ•´å‚æ•°
            if åˆå¹¶ç­–ç•¥ == "ä¿å®ˆåˆå¹¶":
                params["min_words"] = max(1, params["min_words"] - 1)
                params["time_gap"] = int(params["time_gap"] * 0.7)
            elif åˆå¹¶ç­–ç•¥ == "ç§¯æåˆå¹¶":
                params["min_words"] = params["min_words"] + 1
                params["max_words"] = params["max_words"] + 3
                params["time_gap"] = int(params["time_gap"] * 1.5)
            
            self._optimize_with_params(optimized_data, params)
        
        # ç”Ÿæˆä¼˜åŒ–åçš„æ®µæ•°
        optimized_count = len(optimized_data.segments)
        reduction = original_count - optimized_count
        reduction_pct = (reduction / original_count * 100) if original_count > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        mode_name = ä¼˜åŒ–æ¨¡å¼.split()[0]
        report = self._generate_report(
            mode_name,
            original_count,
            optimized_count,
            reduction,
            reduction_pct,
            optimized_data
        )
        
        print(f"[SubtitleOptimizer] ä¼˜åŒ–å®Œæˆ: {original_count} â†’ {optimized_count} æ®µ (â†“{reduction_pct:.1f}%)")
        
        # ç”Ÿæˆå„ç§æ ¼å¼çš„æ—¶é—´æˆ³æ–‡æœ¬
        text_with_timestamp = self._format_text_with_timestamp(optimized_data)
        text_simple_timestamp = self._format_text_with_simple_timestamp(optimized_data)
        text_srt = self._format_text_srt(optimized_data)
        text_json = self._format_text_json(optimized_data)
        text_csv = self._format_text_csv(optimized_data)
        
        print(f"[SubtitleOptimizer] å·²ç”Ÿæˆ 5 ç§æ ¼å¼åŒ–è¾“å‡º")
        
        return (optimized_data, report, text_with_timestamp, text_simple_timestamp, text_srt, text_json, text_csv)
    
    def _optimize_with_params(self, asr_data: Any, params: dict):
        """
        ä½¿ç”¨æŒ‡å®šå‚æ•°ä¼˜åŒ–å­—å¹•
        
        Args:
            asr_data: ASRData å¯¹è±¡
            params: å‚æ•°å­—å…¸ {min_words, max_words, time_gap}
        """
        segments = asr_data.segments
        i = len(segments) - 1
        
        while i > 0:
            prev_seg = segments[i - 1]
            curr_seg = segments[i]
            
            # è®¡ç®—è¯æ•°
            prev_words = count_words(prev_seg.text)
            curr_words = count_words(curr_seg.text)
            
            # è®¡ç®—æ—¶é—´é—´éš”
            time_gap = abs(curr_seg.start_time - prev_seg.end_time)
            
            # åˆ¤æ–­æ˜¯å¦åˆå¹¶
            punctuation_marks = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
            ends_with_punct = False
            try:
                t = prev_seg.text.strip()
                ends_with_punct = len(t) > 0 and t[-1] in punctuation_marks
            except Exception:
                ends_with_punct = False
            should_merge = (
                prev_words <= params["min_words"] and
                time_gap < params["time_gap"] and
                (prev_words + curr_words) <= params["max_words"] and
                not ends_with_punct
            )
            
            if should_merge:
                # æ‰§è¡Œåˆå¹¶
                try:
                    asr_data.merge_with_next_segment(i - 1)
                except Exception as e:
                    print(f"[SubtitleOptimizer] åˆå¹¶å¤±è´¥: {e}")
            
            i -= 1
    
    def _split_by_punctuation(self, asr_data: Any):
        """
        æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ
        
        åœ¨æ‰€æœ‰æ ‡ç‚¹ç¬¦å·å¤„æ‹†åˆ†å­—å¹•æ®µï¼ˆåŒ…æ‹¬é€—å·ã€å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰ï¼‰
        æ¯ä¸ªæ ‡ç‚¹ç¬¦å·ç»“æŸä¸€ä¸ªæ®µè½ï¼Œä¸”æ ‡ç‚¹ç¬¦å·ä¿ç•™åœ¨æ®µè½æœ«å°¾
        
        Args:
            asr_data: ASRData å¯¹è±¡ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
        """
        from app.core.bk_asr.asr_data import ASRDataSeg
        
        # å®šä¹‰æ‰€æœ‰éœ€è¦æ–­å¥çš„æ ‡ç‚¹ç¬¦å·ï¼ˆä¸­è‹±æ–‡ï¼‰
        # åŒ…å«ï¼šå¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€åˆ†å·ã€é€—å·ã€é¡¿å·ã€å†’å·
        punctuation_marks = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
        
        new_segments = []
        
        for seg in asr_data.segments:
            text = seg.text.strip()
            
            # å¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡
            if not text:
                continue
            
            # æŸ¥æ‰¾æ‰€æœ‰æ ‡ç‚¹ç¬¦å·çš„ä½ç½®
            sentences = []
            current_sentence = ""
            
            for i, char in enumerate(text):
                current_sentence += char
                # å¦‚æœå½“å‰å­—ç¬¦æ˜¯æ ‡ç‚¹ç¬¦å·
                if char in punctuation_marks:
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯å¦ä¹Ÿæ˜¯æ ‡ç‚¹ç¬¦å·ï¼ˆå¤„ç†è¿ç»­æ ‡ç‚¹ï¼Œå¦‚ "..." æˆ– "ï¼Ÿï¼"ï¼‰
                    if i + 1 < len(text) and text[i+1] in punctuation_marks:
                        continue
                    
                    if current_sentence.strip():
                        sentences.append(current_sentence.strip())
                    current_sentence = ""
            
            # æ·»åŠ å‰©ä½™çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            if current_sentence.strip():
                sentences.append(current_sentence.strip())
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç‚¹ç¬¦å·ï¼ˆæˆ–è€…æ•´ä¸ªæ–‡æœ¬å°±æ˜¯ä¸€ä¸ªå¥å­ï¼‰ï¼Œä¿æŒåŸæ ·
            if len(sentences) == 0:
                new_segments.append(seg)
                continue
            elif len(sentences) == 1:
                # åªæœ‰ä¸€å¥è¯ï¼Œä¿æŒåŸæ ·
                new_segments.append(seg)
                continue
            
            # æŒ‰å¥å­æ•°é‡åˆ†é…æ—¶é—´
            segment_duration = seg.end_time - seg.start_time
            total_chars = sum(len(s) for s in sentences)
            
            if total_chars == 0:
                new_segments.append(seg)
                continue
            
            current_time = seg.start_time
            
            for i, sentence in enumerate(sentences):
                # æŒ‰å­—ç¬¦æ•°æ¯”ä¾‹åˆ†é…æ—¶é—´
                sentence_chars = len(sentence)
                sentence_duration = int(segment_duration * sentence_chars / total_chars)
                
                # è®¡ç®—ç»“æŸæ—¶é—´
                if i == len(sentences) - 1:
                    # æœ€åä¸€å¥ï¼šä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´
                    end_time = seg.end_time
                else:
                    end_time = current_time + sentence_duration
                
                # é¿å…æ—¶é—´é‡å 
                if end_time <= current_time:
                    end_time = current_time + 100  # è‡³å°‘100ms
                # è‡ªåŠ¨è¡¥å…¨ç»“å°¾æ ‡ç‚¹
                end_puncts = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
                need_punct = True
                if len(sentence) > 0 and sentence[-1] in end_puncts:
                    need_punct = False
                if need_punct:
                    is_last = (i == len(sentences) - 1)
                    q_words = ["å—","ä¹ˆ","ï¼Ÿ","?","ä¸ºä½•","ä¸ºä»€ä¹ˆ","æ€æ ·","æ€ä¹ˆ","æ˜¯å¦","æ˜¯ä¸æ˜¯"]
                    e_words = ["!","ï¼","å¤ª","çœŸ","éå¸¸","æå…¶","ç‰¹åˆ«","å¥½æ£’","éœ‡æ’¼","æƒŠäºº"]
                    has_q = any(w in sentence for w in q_words)
                    has_e = any(w in sentence for w in e_words)
                    ascii_ratio = sum(1 for c in sentence if c.isascii())/max(len(sentence),1)
                    if ascii_ratio > 0.5:
                        add_p = "?" if has_q else ("!" if has_e else ("." if is_last else ","))
                    else:
                        add_p = "ï¼Ÿ" if has_q else ("ï¼" if has_e else ("ã€‚" if is_last else "ï¼Œ"))
                    sentence = sentence + add_p
                # åˆ›å»ºæ–°çš„å­—å¹•æ®µ
                new_seg = ASRDataSeg(
                    start_time=current_time,
                    end_time=end_time,
                    text=sentence
                )
                
                new_segments.append(new_seg)
                current_time = end_time
        
        # æ›¿æ¢åŸå§‹segments
        asr_data.segments = new_segments
    
    def _split_by_speech_pause(self, asr_data: Any, pause_threshold: int):
        """
        æŒ‰è¯´è¯åœé¡¿åˆ†æ®µ
        
        åŸºäºè¯­éŸ³çš„è‡ªç„¶åœé¡¿ï¼ˆæ—¶é—´é—´éš”ï¼‰æ¥åˆ†æ®µ
        - é—´éš”å¤§äºé˜ˆå€¼ï¼šä¿ç•™æ–­å¥ï¼ˆè¯´è¯æœ‰åœé¡¿ï¼‰
        - é—´éš”å°äºé˜ˆå€¼ï¼šåˆå¹¶ï¼ˆè¿ç»­è¯´è¯ï¼‰ï¼Œé™¤éé‡åˆ°æ ‡ç‚¹ç¬¦å·
        
        Args:
            asr_data: ASRData å¯¹è±¡ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            pause_threshold: åœé¡¿é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰ï¼Œå¤§äºæ­¤å€¼è§†ä¸ºè‡ªç„¶åœé¡¿
        """
        segments = asr_data.segments
        
        # ç»Ÿè®¡ä¿¡æ¯
        merge_count = 0
        keep_count = 0
        punct_break_count = 0
        
        # å®šä¹‰å¼ºåˆ¶åˆ†æ®µçš„æ ‡ç‚¹ç¬¦å·
        break_punct = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€.!?,:;ï¼š'
        
        # ä»åå‘å‰éå†ï¼Œæ–¹ä¾¿åˆ é™¤å’Œåˆå¹¶
        i = len(segments) - 1
        
        while i > 0:
            prev_seg = segments[i - 1]
            curr_seg = segments[i]
            
            # è®¡ç®—ä¸¤æ®µä¹‹é—´çš„æ—¶é—´é—´éš”
            time_gap = curr_seg.start_time - prev_seg.end_time
            
            # æ£€æŸ¥å‰ä¸€æ®µæ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾
            prev_text = prev_seg.text.strip()
            has_break_punct = prev_text and prev_text[-1] in break_punct
            
            # å¦‚æœé—´éš”å°äºé˜ˆå€¼ï¼Œä¸”æ²¡æœ‰æ ‡ç‚¹åˆ†éš”ï¼Œè¯´æ˜æ˜¯è¿ç»­è¯´è¯ï¼Œåº”è¯¥åˆå¹¶
            if time_gap < pause_threshold:
                if has_break_punct:
                    # æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œå¼ºåˆ¶åˆ†æ®µ
                    punct_break_count += 1
                else:
                    try:
                        # åˆå¹¶å‰ä¸€æ®µå’Œå½“å‰æ®µ
                        asr_data.merge_with_next_segment(i - 1)
                        merge_count += 1
                    except Exception as e:
                        print(f"[SubtitleOptimizer] åˆå¹¶å¤±è´¥: {e}")
            else:
                # é—´éš”å¤§äºé˜ˆå€¼ï¼Œä¿ç•™æ–­å¥
                keep_count += 1
            
            i -= 1
        
        print(f"[SubtitleOptimizer] åœé¡¿åˆ†æ: åˆå¹¶äº† {merge_count} å¤„è¿ç»­è¯´è¯, ä¿ç•™äº† {keep_count} å¤„è‡ªç„¶åœé¡¿, {punct_break_count} å¤„æ ‡ç‚¹åˆ†æ®µ")
    
    def _split_by_char_limit(self, asr_data: Any, max_chars: int, threshold: int):
        """
        æŒ‰å­—æ•°é™åˆ¶åˆ†æ®µ
        
        å¤„ç†é€»è¾‘ï¼š
        1. å…ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ
        2. æ£€æŸ¥æ¯æ®µå­—ç¬¦æ•°ï¼š
           - å¦‚æœ â‰¤ thresholdï¼šä¿æŒä¸å˜
           - å¦‚æœ > threshold ä¸” â‰¤ max_charsï¼šå°è¯•åœ¨æ ‡ç‚¹å¤„ä¼˜åŒ–åˆ‡åˆ†
           - å¦‚æœ > max_charsï¼šå¼ºåˆ¶åˆ‡åˆ†ï¼ˆä¼˜å…ˆåœ¨æ ‡ç‚¹/ç©ºæ ¼å¤„ï¼Œå¦åˆ™ç¡¬åˆ‡ï¼‰
        
        Args:
            asr_data: ASRData å¯¹è±¡ï¼ˆä¼šè¢«åŸåœ°ä¿®æ”¹ï¼‰
            max_chars: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°ï¼ˆç¡¬é™åˆ¶ï¼‰
            threshold: è¶…è¿‡æ­¤å€¼æ—¶å¼€å§‹å¯»æ‰¾åˆ†æ®µç‚¹ï¼ˆè½¯é™åˆ¶ï¼‰
        """
        # ç¬¬ä¸€æ­¥ï¼šå…ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ
        print(f"[SubtitleOptimizer] æ­¥éª¤1: æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µ")
        self._split_by_punctuation(asr_data)
        
        # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥å¹¶å¤„ç†è¶…é•¿æ®µè½
        print(f"[SubtitleOptimizer] æ­¥éª¤2: æ£€æŸ¥å­—ç¬¦æ•°é™åˆ¶")
        self._enforce_char_limit(asr_data, max_chars, threshold)

    def _enforce_char_limit(self, asr_data: Any, max_chars: int, threshold: int):
        """
        å¼ºåˆ¶æ‰§è¡Œå­—ç¬¦æ•°é™åˆ¶ï¼ˆä¸é¢„å…ˆæŒ‰æ ‡ç‚¹åˆ†æ®µï¼‰
        """
        from app.core.bk_asr.asr_data import ASRDataSeg
        
        new_segments = []
        split_count = 0
        
        for seg in asr_data.segments:
            text = seg.text.strip()
            char_count = len(text)
            
            # å¦‚æœå­—ç¬¦æ•°åœ¨é˜ˆå€¼å†…ï¼Œç›´æ¥ä¿ç•™
            if char_count <= threshold:
                new_segments.append(seg)
                continue
            
            # å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œéœ€è¦åˆ†æ®µ
            if char_count > max_chars:
                # è¶…è¿‡ç¡¬é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†æ®µ
                print(f"[SubtitleOptimizer]   è¶…é•¿æ®µè½({char_count}å­—): '{text[:20]}...' - å¼ºåˆ¶åˆ‡åˆ†")
                sub_segs = self._force_split_segment(seg, max_chars)
                new_segments.extend(sub_segs)
                split_count += len(sub_segs) - 1
            else:
                # åœ¨é˜ˆå€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼Œå°è¯•æ™ºèƒ½åˆ†æ®µ
                print(f"[SubtitleOptimizer]   åé•¿æ®µè½({char_count}å­—): '{text[:20]}...' - å°è¯•ä¼˜åŒ–")
                sub_segs = self._smart_split_segment(seg, threshold, max_chars)
                new_segments.extend(sub_segs)
                if len(sub_segs) > 1:
                    split_count += len(sub_segs) - 1
        
        # æ›¿æ¢åŸå§‹segments
        asr_data.segments = new_segments
        print(f"[SubtitleOptimizer] å­—æ•°é™åˆ¶åˆ†æ: åˆ‡åˆ†äº† {split_count} ä¸ªè¶…é•¿æ®µè½")
    
    def _smart_split_segment(self, seg: Any, threshold: int, max_chars: int) -> list:
        """
        æ™ºèƒ½åˆ‡åˆ†æ®µè½ï¼ˆåœ¨é˜ˆå€¼å’Œæœ€å¤§å€¼ä¹‹é—´ï¼‰
        
        å°è¯•åœ¨åˆé€‚çš„æ ‡ç‚¹ç¬¦å·å¤„åˆ‡åˆ†ï¼Œä½¿æ¯æ®µé•¿åº¦æ›´åˆç†
        
        Args:
            seg: è¦åˆ‡åˆ†çš„å­—å¹•æ®µ
            threshold: è½¯é™åˆ¶
            max_chars: ç¡¬é™åˆ¶
            
        Returns:
            åˆ‡åˆ†åçš„å­—å¹•æ®µåˆ—è¡¨
        """
        from app.core.bk_asr.asr_data import ASRDataSeg
        
        text = seg.text.strip()
        
        # å®šä¹‰ä¸»è¦æ ‡ç‚¹ç¬¦å·ï¼ˆä¼˜å…ˆåœ¨è¿™äº›ä½ç½®åˆ‡åˆ†ï¼‰
        major_punctuation = 'ã€‚ï¼ï¼Ÿï¼›.!?;'
        # æ¬¡è¦æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚æœæ²¡æœ‰ä¸»è¦æ ‡ç‚¹ï¼Œå¯ä»¥åœ¨è¿™é‡Œåˆ‡åˆ†ï¼‰
        minor_punctuation = 'ï¼Œã€ï¼š,:ï¼š'
        
        # å¯»æ‰¾æœ€ä½³åˆ‡åˆ†ç‚¹
        best_split_pos = -1
        
        # ä¼˜å…ˆåœ¨ threshold é™„è¿‘å¯»æ‰¾ä¸»è¦æ ‡ç‚¹
        for i in range(threshold - 5, min(threshold + 5, len(text))):
            if i > 0 and i < len(text) and text[i] in major_punctuation:
                best_split_pos = i + 1  # æ ‡ç‚¹ååˆ‡åˆ†
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦æ ‡ç‚¹ï¼Œå¯»æ‰¾æ¬¡è¦æ ‡ç‚¹
        if best_split_pos == -1:
            for i in range(threshold - 5, min(threshold + 5, len(text))):
                if i > 0 and i < len(text) and text[i] in minor_punctuation:
                    best_split_pos = i + 1
                    break
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°±åœ¨ threshold ä½ç½®å¼ºåˆ¶åˆ‡åˆ†
        if best_split_pos == -1:
            # å°è¯•åœ¨ç©ºæ ¼å¤„åˆ‡åˆ†
            for i in range(threshold - 3, min(threshold + 3, len(text))):
                if i > 0 and i < len(text) and text[i] == ' ':
                    best_split_pos = i + 1
                    break
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œåœ¨ threshold ä½ç½®ç¡¬åˆ‡
        if best_split_pos == -1:
            best_split_pos = threshold
        
        # å¦‚æœåˆ‡åˆ†ç‚¹å¤ªé åï¼Œç›´æ¥è¿”å›åŸæ®µè½
        if best_split_pos >= len(text) - 3:
            return [seg]
        
        # æ‰§è¡Œåˆ‡åˆ†
        part1_text = text[:best_split_pos].strip()
        part2_text = text[best_split_pos:].strip()
        
        if not part1_text or not part2_text:
            return [seg]
        
        # è®¡ç®—æ—¶é—´åˆ†é…ï¼ˆæŒ‰å­—ç¬¦æ•°æ¯”ä¾‹ï¼‰
        total_duration = seg.end_time - seg.start_time
        part1_ratio = len(part1_text) / len(text)
        part1_duration = int(total_duration * part1_ratio)
        
        mid_time = seg.start_time + part1_duration
        
        # åˆ›å»ºä¸¤ä¸ªæ–°æ®µï¼ˆASRDataSeg å‚æ•°é¡ºåºï¼štext, start_time, end_timeï¼‰
        seg1 = ASRDataSeg(
            text=part1_text,
            start_time=seg.start_time,
            end_time=mid_time
        )
        
        seg2 = ASRDataSeg(
            text=part2_text,
            start_time=mid_time,
            end_time=seg.end_time
        )
        
        return [seg1, seg2]
    
    def _force_split_segment(self, seg: Any, max_chars: int) -> list:
        """
        å¼ºåˆ¶åˆ‡åˆ†è¶…é•¿æ®µè½
        
        æŒ‰ç…§ max_chars åˆ‡åˆ†ï¼Œä¼˜å…ˆåœ¨æ ‡ç‚¹ç¬¦å·æˆ–ç©ºæ ¼å¤„åˆ‡åˆ†
        
        Args:
            seg: è¦åˆ‡åˆ†çš„å­—å¹•æ®µ
            max_chars: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            åˆ‡åˆ†åçš„å­—å¹•æ®µåˆ—è¡¨
        """
        from app.core.bk_asr.asr_data import ASRDataSeg
        
        text = seg.text.strip()
        total_duration = seg.end_time - seg.start_time
        
        # æ‰€æœ‰æ ‡ç‚¹ç¬¦å·
        all_punctuation = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
        
        segments = []
        current_pos = 0
        current_time = seg.start_time
        
        while current_pos < len(text):
            # ç¡®å®šæœ¬æ®µçš„ç»“æŸä½ç½®
            end_pos = min(current_pos + max_chars, len(text))
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ®µï¼Œå°è¯•åœ¨æ ‡ç‚¹æˆ–ç©ºæ ¼å¤„åˆ‡åˆ†
            if end_pos < len(text):
                # å‘å‰æœç´¢æœ€è¿‘çš„æ ‡ç‚¹ç¬¦å·
                best_cut = end_pos
                for i in range(end_pos - 1, max(current_pos + max_chars // 2, end_pos - 10), -1):
                    if text[i] in all_punctuation:
                        best_cut = i + 1  # æ ‡ç‚¹ååˆ‡åˆ†
                        break
                    elif text[i] == ' ':
                        best_cut = i + 1  # ç©ºæ ¼ååˆ‡åˆ†ï¼ˆå¦‚æœæ²¡æ‰¾åˆ°æ ‡ç‚¹ï¼‰
                
                end_pos = best_cut
            
            # æå–æ–‡æœ¬
            segment_text = text[current_pos:end_pos].strip()
            
            if segment_text:
                # è®¡ç®—æ—¶é—´ï¼ˆæŒ‰å­—ç¬¦æ•°æ¯”ä¾‹ï¼‰
                char_ratio = len(segment_text) / len(text)
                segment_duration = int(total_duration * char_ratio)
                segment_end_time = current_time + segment_duration
                
                # æœ€åä¸€æ®µä½¿ç”¨åŸå§‹ç»“æŸæ—¶é—´
                if end_pos >= len(text):
                    segment_end_time = seg.end_time
                
                # åˆ›å»ºæ–°æ®µï¼ˆASRDataSeg å‚æ•°é¡ºåºï¼štext, start_time, end_timeï¼‰
                new_seg = ASRDataSeg(
                    text=segment_text,
                    start_time=current_time,
                    end_time=segment_end_time
                )
                
                segments.append(new_seg)
                current_time = segment_end_time
            
            current_pos = end_pos
        
        return segments if segments else [seg]
    
    def _append_punctuation(self, asr_data: Any):
        end_puncts = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
        q_words = ["å—","ä¹ˆ","ä¸ºä½•","ä¸ºä»€ä¹ˆ","æ€æ ·","æ€ä¹ˆ","æ˜¯å¦","æ˜¯ä¸æ˜¯","èƒ½å¦","å¯å¦","ï¼Ÿ","?"]
        e_words = ["ï¼","!","å¤ª","çœŸ","éå¸¸","æå…¶","ç‰¹åˆ«","å¥½æ£’","éœ‡æ’¼","æƒŠäºº","å‰å®³","ç²¾å½©","å¿«çœ‹","æ³¨æ„"]
        for seg in asr_data.segments:
            t = seg.text.strip()
            if not t:
                continue
            if t[-1] in end_puncts:
                continue
            has_q = any(w in t for w in q_words)
            has_e = any(w in t for w in e_words)
            ascii_ratio = sum(1 for c in t if c.isascii())/max(len(t),1)
            if ascii_ratio > 0.5:
                add_p = "?" if has_q else ("!" if has_e else ".")
            else:
                add_p = "ï¼Ÿ" if has_q else ("ï¼" if has_e else "ã€‚")
            seg.text = t + add_p
    
    def _append_punctuation_by_pause(self, asr_data: Any, pause_threshold_ms: int):
        end_puncts = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œã€ï¼š.!?;,:'
        q_words = ["å—","ä¹ˆ","ä¸ºä½•","ä¸ºä»€ä¹ˆ","æ€æ ·","æ€ä¹ˆ","æ˜¯å¦","æ˜¯ä¸æ˜¯","èƒ½å¦","å¯å¦","ï¼Ÿ","?"]
        e_words = ["ï¼","!","å¤ª","çœŸ","éå¸¸","æå…¶","ç‰¹åˆ«","å¥½æ£’","éœ‡æ’¼","æƒŠäºº","å‰å®³","ç²¾å½©","å¿«çœ‹","æ³¨æ„"]
        segs = asr_data.segments
        n = len(segs)
        for i, seg in enumerate(segs):
            text = seg.text.strip()
            if not text:
                continue
            if text[-1] in end_puncts:
                continue
            next_gap = None
            if i < n - 1:
                next_gap = segs[i+1].start_time - seg.end_time
            # é—®å¥/æ„Ÿå¹ä¼˜å…ˆ
            has_q = any(w in text for w in q_words)
            has_e = any(w in text for w in e_words)
            ascii_ratio = sum(1 for c in text if c.isascii())/max(len(text),1)
            if ascii_ratio > 0.5:
                comma = ","
                dot = "."
                q = "?"
                e = "!"
            else:
                comma = "ï¼Œ"
                dot = "ã€‚"
                q = "ï¼Ÿ"
                e = "ï¼"
            if has_q:
                seg.text = text + q
            elif has_e:
                seg.text = text + e
            else:
                # æ ¹æ®è‡ªç„¶åœé¡¿åˆ¤æ–­é€—å·/å¥å·
                if next_gap is not None and next_gap < pause_threshold_ms:
                    seg.text = text + comma
                else:
                    seg.text = text + dot
    
    def _generate_report(
        self, 
        mode: str, 
        original: int, 
        optimized: int, 
        reduction: int, 
        pct: float,
        asr_data: Any
    ) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        
        # è®¡ç®—å¹³å‡é•¿åº¦
        if optimized > 0:
            total_duration = sum(
                seg.end_time - seg.start_time 
                for seg in asr_data.segments
            )
            avg_duration = total_duration / optimized / 1000  # è½¬æ¢ä¸ºç§’
        else:
            avg_duration = 0
        
        # ç»Ÿè®¡è¯æ•°åˆ†å¸ƒ
        word_counts = [count_words(seg.text) for seg in asr_data.segments]
        short_count = sum(1 for w in word_counts if w <= 3)
        medium_count = sum(1 for w in word_counts if 4 <= w <= 10)
        long_count = sum(1 for w in word_counts if w > 10)
        
        report = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         å­—å¹•ä¼˜åŒ–æŠ¥å‘Š
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ ä¼˜åŒ–æ¨¡å¼: {mode}

ğŸ“Š æ®µæ•°ç»Ÿè®¡:
  â€¢ ä¼˜åŒ–å‰: {original} æ®µ
  â€¢ ä¼˜åŒ–å: {optimized} æ®µ
  â€¢ å‡å°‘: {reduction} æ®µ (â†“{pct:.1f}%)

â±ï¸  å¹³å‡æ—¶é•¿:
  â€¢ æ¯æ®µ: {avg_duration:.2f} ç§’

ğŸ“ è¯æ•°åˆ†å¸ƒ:
  â€¢ çŸ­å¥ (â‰¤3è¯): {short_count} æ®µ ({short_count/optimized*100:.1f}%)
  â€¢ ä¸­å¥ (4-10è¯): {medium_count} æ®µ ({medium_count/optimized*100:.1f}%)
  â€¢ é•¿å¥ (>10è¯): {long_count} æ®µ ({long_count/optimized*100:.1f}%)

âœ… ä¼˜åŒ–çŠ¶æ€: å®Œæˆ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return report.strip()
    
    @staticmethod
    def _format_text_with_timestamp(asr_data: Any) -> str:
        """
        å°† ASRData æ ¼å¼åŒ–ä¸ºå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬ï¼ˆå®Œæ•´æ ¼å¼ï¼‰
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        [00:00:01.000 --> 00:00:03.000] ä½ å¥½ä¸–ç•Œ
        [00:00:03.500 --> 00:00:05.200] è¿™æ˜¯ç¬¬äºŒå¥è¯
        
        Args:
            asr_data: ASRData å¯¹è±¡
            
        Returns:
            å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        lines = []
        
        for segment in asr_data.segments:
            # æ ¼å¼åŒ–æ—¶é—´æˆ³ (æ¯«ç§’ -> HH:MM:SS.mmm)
            start_ms = segment.start_time
            end_ms = segment.end_time
            
            # è½¬æ¢ä¸º HH:MM:SS.mmm æ ¼å¼
            def ms_to_timestamp(ms: int) -> str:
                total_seconds = ms // 1000
                milliseconds = ms % 1000
                hours = total_seconds // 3600
                minutes = (total_seconds % 3600) // 60
                seconds = total_seconds % 60
                return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"
            
            start_time_str = ms_to_timestamp(start_ms)
            end_time_str = ms_to_timestamp(end_ms)
            
            # æ ¼å¼: [å¼€å§‹æ—¶é—´ --> ç»“æŸæ—¶é—´] æ–‡æœ¬
            line = f"[{start_time_str} --> {end_time_str}] {segment.text}"
            lines.append(line)
        
        return "\n".join(lines)
    
    @staticmethod
    def _format_text_with_simple_timestamp(asr_data: Any) -> str:
        """
        å°† ASRData æ ¼å¼åŒ–ä¸ºç®€æ´æ—¶é—´æˆ³æ–‡æœ¬ï¼ˆç§’æ•°æ ¼å¼ï¼‰
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        (0.0, 0.26) å¤§éš¾ï¼Œ
        (0.3, 1.4) æˆ‘æ¥å‚åŠ æŠ•ç¨¿äº†ï¼Œ
        (1.5, 2.26) å¿«å‘Šè¯‰æˆ‘ï¼Œ
        
        Args:
            asr_data: ASRData å¯¹è±¡
            
        Returns:
            ç®€æ´æ—¶é—´æˆ³æ–‡æœ¬å­—ç¬¦ä¸²
        """
        lines = []
        
        for segment in asr_data.segments:
            # è½¬æ¢ä¸ºç§’ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
            start_seconds = segment.start_time / 1000.0
            end_seconds = segment.end_time / 1000.0
            
            # æ ¼å¼: (å¼€å§‹ç§’, ç»“æŸç§’) æ–‡æœ¬
            # å»é™¤ä¸å¿…è¦çš„å°æ•°ä½ï¼ˆå¦‚æœæ˜¯æ•´æ•°å°±æ˜¾ç¤ºæ•´æ•°ï¼‰
            start_str = f"{start_seconds:.2f}".rstrip('0').rstrip('.')
            end_str = f"{end_seconds:.2f}".rstrip('0').rstrip('.')
            
            line = f"({start_str}, {end_str}) {segment.text}"
            lines.append(line)
        
        return "\n".join(lines)
    
    @staticmethod
    def _format_text_srt(asr_data: Any) -> str:
        """
        å°† ASRData æ ¼å¼åŒ–ä¸º SRT å­—å¹•æ ¼å¼
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        1
        00:00:00,000 --> 00:00:00,260
        å¤§éš¾ï¼Œ
        
        2
        00:00:00,300 --> 00:00:01,400
        æˆ‘æ¥å‚åŠ æŠ•ç¨¿äº†ï¼Œ
        
        Args:
            asr_data: ASRData å¯¹è±¡
            
        Returns:
            SRT æ ¼å¼å­—ç¬¦ä¸²
        """
        lines = []
        
        for i, segment in enumerate(asr_data.segments, 1):
            # è½¬æ¢ä¸º SRT æ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)
            def ms_to_srt_time(ms: int) -> str:
                total_seconds = ms // 1000
                milliseconds = ms % 1000
                hours = total_seconds // 3600
                minutes = (total_seconds % 3600) // 60
                seconds = total_seconds % 60
                return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"
            
            start_time = ms_to_srt_time(segment.start_time)
            end_time = ms_to_srt_time(segment.end_time)
            
            # SRT æ ¼å¼ï¼šåºå·ã€æ—¶é—´æˆ³ã€æ–‡æœ¬ã€ç©ºè¡Œ
            lines.append(str(i))
            lines.append(f"{start_time} --> {end_time}")
            lines.append(segment.text)
            lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(lines)
    
    @staticmethod
    def _format_text_json(asr_data: Any) -> str:
        """
        å°† ASRData æ ¼å¼åŒ–ä¸º JSON æ ¼å¼
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        [
          {
            "index": 1,
            "start": 0.0,
            "end": 0.26,
            "duration": 0.26,
            "text": "å¤§éš¾ï¼Œ"
          },
          ...
        ]
        
        Args:
            asr_data: ASRData å¯¹è±¡
            
        Returns:
            JSON æ ¼å¼å­—ç¬¦ä¸²
        """
        import json
        
        segments = []
        for i, segment in enumerate(asr_data.segments, 1):
            start_seconds = segment.start_time / 1000.0
            end_seconds = segment.end_time / 1000.0
            
            segments.append({
                "index": i,
                "start": round(start_seconds, 3),
                "end": round(end_seconds, 3),
                "duration": round(end_seconds - start_seconds, 3),
                "text": segment.text
            })
        
        return json.dumps(segments, ensure_ascii=False, indent=2)
    
    @staticmethod
    def _format_text_csv(asr_data: Any) -> str:
        """
        å°† ASRData æ ¼å¼åŒ–ä¸º CSV æ ¼å¼
        
        æ ¼å¼ç¤ºä¾‹ï¼š
        index,start,end,duration,text
        1,0.0,0.26,0.26,"å¤§éš¾ï¼Œ"
        2,0.3,1.4,1.1,"æˆ‘æ¥å‚åŠ æŠ•ç¨¿äº†ï¼Œ"
        3,1.5,2.26,0.76,"å¿«å‘Šè¯‰æˆ‘ï¼Œ"
        
        Args:
            asr_data: ASRData å¯¹è±¡
            
        Returns:
            CSV æ ¼å¼å­—ç¬¦ä¸²
        """
        import csv
        from io import StringIO
        
        output = StringIO()
        writer = csv.writer(output)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow(["index", "start", "end", "duration", "text"])
        
        # å†™å…¥æ•°æ®
        for i, segment in enumerate(asr_data.segments, 1):
            start_seconds = round(segment.start_time / 1000.0, 3)
            end_seconds = round(segment.end_time / 1000.0, 3)
            duration = round(end_seconds - start_seconds, 3)
            
            writer.writerow([i, start_seconds, end_seconds, duration, segment.text])
        
        return output.getvalue()


NODE_CLASS_MAPPINGS = {
    "SubtitleOptimizerNode": SubtitleOptimizerNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SubtitleOptimizerNode": "å­—å¹•ä¼˜åŒ– (æ™ºèƒ½åˆ†æ®µ)"
}
